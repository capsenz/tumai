{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center;\">\n",
    "<img src=\"https://makeathon.tum-ai.com/assets/logo.svg\" class=\"center\">\n",
    "</p>\n",
    "\n",
    "# Welcome to the Aleph Alpha workshop at the TUM.ai makeathon!\n",
    "\n",
    "Today we will run through a little tutorial showing you how you can combine the capabilities of Large Language Models, a document database and explain the output to the user. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Idea\n",
    "\n",
    "We have database full of documents and want to work on those documents. To make those documents retrievable, we have to embed them.\n",
    "<p style=\"text-align:center;\">\n",
    "<img src=\"https://docs.aleph-alpha.com/assets/images/asymmetric_embedding_db-1958346838d50e1dd92a88b5f80b3d40.png\" class=\"center\" width=\"900\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich import print\n",
    "docs = [\n",
    "    \"The Makeathon is a 2-day hackathon for motivated students and young professionals to develop innovative AI solutions on real-world business cases presented by industry leaders. Innovators of tomorrow work together in interdisciplinary teams to develop a prototype for challenges centred around the theme of AI for everyone. As Germany's leading AI student initiative, we drive positive social impact through interdisciplinary projects.\\n\\nThe Makeathon seeks to achieve this by providing talks of leading industry speakers, hands-on challenges, networking opportunities with like-minded people, industry leaders, start-ups and research organizations. This year's Makeathon is in-person on TUM's campus in Garching during the last week of April. We invite you to explore the possibilities of AI and create innovative solutions that can positively impact everyone.\",\n",
    "    \"The Technical University of Munich (TUM or TU Munich; German: Technische Universität München) is a public research university in Munich, Germany. It specializes in engineering, technology, medicine, and applied and natural sciences. Established in 1868 by King Ludwig II of Bavaria, the university now has additional campuses in Garching, Freising, Heilbronn, Straubing, and Singapore, with the Garching campus being its largest.\\n\\nTUM is organized into eight schools and departments, and is supported by numerous research centers. It is one of the largest universities in Germany, with 50,484 students and an annual budget of €1,770.3 million (including the university hospital). A University of Excellence under the German Universities Excellence Initiative, TUM is considered the top university in Germany according to major rankings as of 2022 and is among the leading universities in the European Union. Its researchers and alumni include 18 Nobel laureates and 23 Leibniz Prize winners.\",\n",
    "    \"German AI startup Aleph Alpha has now raised €23 million/$27 million in a Series A funding co-led by Earlybird VC, Lakestar and UVC Partners. Following a seed round of €5.3 million from LEA Partners, 468 Capital and Cavalry Ventures in November 2020, Aleph Alpha has now raised a total of €28.3 million ($33.3 million). Headquartered in Heidelberg, Germany, Aleph Alpha was founded in 2019 by Jonas Andrulis and co-founder Samuel Weinbach.\\n\\nThe idea behind Aleph Alpha is that it researches, develops and “operationalizes” large AI systems toward generalizable AI, offering GPT-3-like text, vision and strategy AI models. The platform will run a public API enabling public and private sectors to run their own AI experiments and develop new business models. The team says it will have a strong commitment to open-source communities (such as Eleuther.AI), academic partnerships, and will be pushing “European values and ethical standards,” it says, “supporting fairer access to modern AI research— aimed at counteracting ongoing ‘de-democratization,’ monopolization, and loss of control or transparency.” The move is clearly meant to be a stake in the ground in the international world of AI development.\",\n",
    "    \"A large language model (LLM) is a language model consisting of a neural network with many parameters (typically billions of weights or more), trained on large quantities of unlabeled text using self-supervised learning. LLMs emerged around 2018 and perform well at a wide variety of tasks. This has shifted the focus of natural language processing research away from the previous paradigm of training specialized supervised models for specific tasks.\\n\\nThough the term large language model has no formal definition, it often refers to deep learning models having a parameter count on the order of billions or more. LLMs are general purpose models which excel at a wide range of tasks, as opposed to being trained for one specific task (such as sentiment analysis, named entity recognition, or mathematical reasoning). The skill with which they accomplish tasks, and the range of tasks at which they are capable, seems to be a function of the amount of resources (data, parameter-size, computing power) devoted to them, in a way that is not dependent on additional breakthroughs in design.\"\n",
    "]\n",
    "\n",
    "# function to split the texts at the double newline \n",
    "def splitting(texts):\n",
    "    final_texts = []\n",
    "\n",
    "    for text in texts:\n",
    "        splitted_text = text.split(\"\\n\\n\")\n",
    "        final_texts.extend(splitted_text)\n",
    "\n",
    "    return final_texts\n",
    "\n",
    "splitted_docs = splitting(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">\"The Makeathon is a 2-day hackathon for motivated students and young professionals to develop innovative AI </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">solutions on real-world business cases presented by industry leaders. Innovators of tomorrow work together in </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">interdisciplinary teams to develop a prototype for challenges centred around the theme of AI for everyone. As </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Germany's leading AI student initiative, we drive positive social impact through interdisciplinary projects.\"</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">\"The Makeathon seeks to achieve this by providing talks of leading industry speakers, hands-on challenges, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">networking opportunities with like-minded people, industry leaders, start-ups and research organizations. This </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">year's Makeathon is in-person on TUM's campus in Garching during the last week of April. We invite you to explore </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the possibilities of AI and create innovative solutions that can positively impact everyone.\"</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'The Technical University of Munich (TUM or TU Munich; German: Technische Universität München) is a public </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">research university in Munich, Germany. It specializes in engineering, technology, medicine, and applied and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">natural sciences. Established in 1868 by King Ludwig II of Bavaria, the university now has additional campuses in </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Garching, Freising, Heilbronn, Straubing, and Singapore, with the Garching campus being its largest.'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'TUM is organized into eight schools and departments, and is supported by numerous research centers. It is one </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">of the largest universities in Germany, with 50,484 students and an annual budget of €1,770.3 million (including </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the university hospital). A University of Excellence under the German Universities Excellence Initiative, TUM is </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">considered the top university in Germany according to major rankings as of 2022 and is among the leading </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">universities in the European Union. Its researchers and alumni include 18 Nobel laureates and 23 Leibniz Prize </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">winners.'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'German AI startup Aleph Alpha has now raised €23 million/$27 million in a Series A funding co-led by Earlybird</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">VC, Lakestar and UVC Partners. Following a seed round of €5.3 million from LEA Partners, 468 Capital and Cavalry </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Ventures in November 2020, Aleph Alpha has now raised a total of €28.3 million ($33.3 million). Headquartered in </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Heidelberg, Germany, Aleph Alpha was founded in 2019 by Jonas Andrulis and co-founder Samuel Weinbach.'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'The idea behind Aleph Alpha is that it researches, develops and “operationalizes” large AI systems toward </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">generalizable AI, offering GPT-3-like text, vision and strategy AI models. The platform will run a public API </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">enabling public and private sectors to run their own AI experiments and develop new business models. The team says </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">it will have a strong commitment to open-source communities (such as Eleuther.AI), academic partnerships, and will </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">be pushing “European values and ethical standards,” it says, “supporting fairer access to modern AI research— aimed</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">at counteracting ongoing ‘de-democratization,’ monopolization, and loss of control or transparency.” The move is </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">clearly meant to be a stake in the ground in the international world of AI development.'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'A large language model (LLM) is a language model consisting of a neural network with many parameters </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">(typically billions of weights or more), trained on large quantities of unlabeled text using self-supervised </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">learning. LLMs emerged around 2018 and perform well at a wide variety of tasks. This has shifted the focus of </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">natural language processing research away from the previous paradigm of training specialized supervised models for </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">specific tasks.'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'Though the term large language model has no formal definition, it often refers to deep learning models having </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">a parameter count on the order of billions or more. LLMs are general purpose models which excel at a wide range of </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">tasks, as opposed to being trained for one specific task (such as sentiment analysis, named entity recognition, or </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">mathematical reasoning). The skill with which they accomplish tasks, and the range of tasks at which they are </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">capable, seems to be a function of the amount of resources (data, parameter-size, computing power) devoted to them,</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">in a way that is not dependent on additional breakthroughs in design.'</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\n",
       "    \u001b[32m\"The Makeathon is a 2-day hackathon for motivated students and young professionals to develop innovative AI \u001b[0m\n",
       "\u001b[32msolutions on real-world business cases presented by industry leaders. Innovators of tomorrow work together in \u001b[0m\n",
       "\u001b[32minterdisciplinary teams to develop a prototype for challenges centred around the theme of AI for everyone. As \u001b[0m\n",
       "\u001b[32mGermany's leading AI student initiative, we drive positive social impact through interdisciplinary projects.\"\u001b[0m,\n",
       "    \u001b[32m\"The Makeathon seeks to achieve this by providing talks of leading industry speakers, hands-on challenges, \u001b[0m\n",
       "\u001b[32mnetworking opportunities with like-minded people, industry leaders, start-ups and research organizations. This \u001b[0m\n",
       "\u001b[32myear's Makeathon is in-person on TUM's campus in Garching during the last week of April. We invite you to explore \u001b[0m\n",
       "\u001b[32mthe possibilities of AI and create innovative solutions that can positively impact everyone.\"\u001b[0m,\n",
       "    \u001b[32m'The Technical University of Munich \u001b[0m\u001b[32m(\u001b[0m\u001b[32mTUM or TU Munich; German: Technische Universität München\u001b[0m\u001b[32m)\u001b[0m\u001b[32m is a public \u001b[0m\n",
       "\u001b[32mresearch university in Munich, Germany. It specializes in engineering, technology, medicine, and applied and \u001b[0m\n",
       "\u001b[32mnatural sciences. Established in 1868 by King Ludwig II of Bavaria, the university now has additional campuses in \u001b[0m\n",
       "\u001b[32mGarching, Freising, Heilbronn, Straubing, and Singapore, with the Garching campus being its largest.'\u001b[0m,\n",
       "    \u001b[32m'TUM is organized into eight schools and departments, and is supported by numerous research centers. It is one \u001b[0m\n",
       "\u001b[32mof the largest universities in Germany, with 50,484 students and an annual budget of €1,770.3 million \u001b[0m\u001b[32m(\u001b[0m\u001b[32mincluding \u001b[0m\n",
       "\u001b[32mthe university hospital\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. A University of Excellence under the German Universities Excellence Initiative, TUM is \u001b[0m\n",
       "\u001b[32mconsidered the top university in Germany according to major rankings as of 2022 and is among the leading \u001b[0m\n",
       "\u001b[32muniversities in the European Union. Its researchers and alumni include 18 Nobel laureates and 23 Leibniz Prize \u001b[0m\n",
       "\u001b[32mwinners.'\u001b[0m,\n",
       "    \u001b[32m'German AI startup Aleph Alpha has now raised €23 million/$27 million in a Series A funding co-led by Earlybird\u001b[0m\n",
       "\u001b[32mVC, Lakestar and UVC Partners. Following a seed round of €5.3 million from LEA Partners, 468 Capital and Cavalry \u001b[0m\n",
       "\u001b[32mVentures in November 2020, Aleph Alpha has now raised a total of €28.3 million \u001b[0m\u001b[32m(\u001b[0m\u001b[32m$33.3 million\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. Headquartered in \u001b[0m\n",
       "\u001b[32mHeidelberg, Germany, Aleph Alpha was founded in 2019 by Jonas Andrulis and co-founder Samuel Weinbach.'\u001b[0m,\n",
       "    \u001b[32m'The idea behind Aleph Alpha is that it researches, develops and “operationalizes” large AI systems toward \u001b[0m\n",
       "\u001b[32mgeneralizable AI, offering GPT-3-like text, vision and strategy AI models. The platform will run a public API \u001b[0m\n",
       "\u001b[32menabling public and private sectors to run their own AI experiments and develop new business models. The team says \u001b[0m\n",
       "\u001b[32mit will have a strong commitment to open-source communities \u001b[0m\u001b[32m(\u001b[0m\u001b[32msuch as Eleuther.AI\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, academic partnerships, and will \u001b[0m\n",
       "\u001b[32mbe pushing “European values and ethical standards,” it says, “supporting fairer access to modern AI research— aimed\u001b[0m\n",
       "\u001b[32mat counteracting ongoing ‘de-democratization,’ monopolization, and loss of control or transparency.” The move is \u001b[0m\n",
       "\u001b[32mclearly meant to be a stake in the ground in the international world of AI development.'\u001b[0m,\n",
       "    \u001b[32m'A large language model \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLLM\u001b[0m\u001b[32m)\u001b[0m\u001b[32m is a language model consisting of a neural network with many parameters \u001b[0m\n",
       "\u001b[32m(\u001b[0m\u001b[32mtypically billions of weights or more\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, trained on large quantities of unlabeled text using self-supervised \u001b[0m\n",
       "\u001b[32mlearning. LLMs emerged around 2018 and perform well at a wide variety of tasks. This has shifted the focus of \u001b[0m\n",
       "\u001b[32mnatural language processing research away from the previous paradigm of training specialized supervised models for \u001b[0m\n",
       "\u001b[32mspecific tasks.'\u001b[0m,\n",
       "    \u001b[32m'Though the term large language model has no formal definition, it often refers to deep learning models having \u001b[0m\n",
       "\u001b[32ma parameter count on the order of billions or more. LLMs are general purpose models which excel at a wide range of \u001b[0m\n",
       "\u001b[32mtasks, as opposed to being trained for one specific task \u001b[0m\u001b[32m(\u001b[0m\u001b[32msuch as sentiment analysis, named entity recognition, or \u001b[0m\n",
       "\u001b[32mmathematical reasoning\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. The skill with which they accomplish tasks, and the range of tasks at which they are \u001b[0m\n",
       "\u001b[32mcapable, seems to be a function of the amount of resources \u001b[0m\u001b[32m(\u001b[0m\u001b[32mdata, parameter-size, computing power\u001b[0m\u001b[32m)\u001b[0m\u001b[32m devoted to them,\u001b[0m\n",
       "\u001b[32min a way that is not dependent on additional breakthroughs in design.'\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(splitted_docs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from aleph_alpha_client import Client, Prompt, SemanticRepresentation, SemanticEmbeddingRequest\n",
    "\n",
    "client = Client(token=os.getenv(\"AA_TOKEN\"))\n",
    "\n",
    "# embedding the splits\n",
    "embeddings = []\n",
    "for text in splitted_docs:\n",
    "    embedding_params = {\n",
    "            \"prompt\": Prompt.from_text(text),\n",
    "            \"representation\": SemanticRepresentation.Document,\n",
    "            \"compress_to_size\": 128,\n",
    "        }\n",
    "    req = SemanticEmbeddingRequest(**embedding_params)\n",
    "    response = client.semantic_embed(req, model=\"luminous-base\")\n",
    "    embeddings.append(response.embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
       "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-2.1875</span>,\n",
       "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.5625</span>,\n",
       "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.75</span>,\n",
       "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.5546875</span>,\n",
       "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-2.65625</span>,\n",
       "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-5.75</span>,\n",
       "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.9921875</span>,\n",
       "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.03125</span>,\n",
       "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-5.15625</span>,\n",
       "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-2.015625</span>,\n",
       "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.796875</span>,\n",
       "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.171875</span>,\n",
       "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.59375</span>,\n",
       "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-3.46875</span>,\n",
       "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.1640625</span>,\n",
       "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-5.8125</span>,\n",
       "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.6796875</span>,\n",
       "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.98828125</span>,\n",
       "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2.75</span>,\n",
       "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2.53125</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\n",
       "    \u001b[1;36m-2.1875\u001b[0m,\n",
       "    \u001b[1;36m-0.5625\u001b[0m,\n",
       "    \u001b[1;36m-0.75\u001b[0m,\n",
       "    \u001b[1;36m-0.5546875\u001b[0m,\n",
       "    \u001b[1;36m-2.65625\u001b[0m,\n",
       "    \u001b[1;36m-5.75\u001b[0m,\n",
       "    \u001b[1;36m-1.9921875\u001b[0m,\n",
       "    \u001b[1;36m-1.03125\u001b[0m,\n",
       "    \u001b[1;36m-5.15625\u001b[0m,\n",
       "    \u001b[1;36m-2.015625\u001b[0m,\n",
       "    \u001b[1;36m0.796875\u001b[0m,\n",
       "    \u001b[1;36m3.171875\u001b[0m,\n",
       "    \u001b[1;36m1.59375\u001b[0m,\n",
       "    \u001b[1;36m-3.46875\u001b[0m,\n",
       "    \u001b[1;36m-1.1640625\u001b[0m,\n",
       "    \u001b[1;36m-5.8125\u001b[0m,\n",
       "    \u001b[1;36m0.6796875\u001b[0m,\n",
       "    \u001b[1;36m-0.98828125\u001b[0m,\n",
       "    \u001b[1;36m2.75\u001b[0m,\n",
       "    \u001b[1;36m2.53125\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(embeddings[0][:20])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up a vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UpdateResult(operation_id=0, status=<UpdateStatus.COMPLETED: 'completed'>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import Distance, VectorParams, Batch\n",
    "\n",
    "q_client = QdrantClient(path=\"test_db\")\n",
    "\n",
    "q_client.recreate_collection(\n",
    "    collection_name=\"test_collection\",\n",
    "    vectors_config=VectorParams(size=128, distance=Distance.COSINE),\n",
    ")\n",
    "\n",
    "ids = list(range(len(splitted_docs)))\n",
    "payloads = [{\"text\": text} for text in splitted_docs]\n",
    "\n",
    "q_client.upsert(\n",
    "     collection_name=\"test_collection\",\n",
    "     points=Batch(\n",
    "     ids=ids,\n",
    "     payloads=payloads,\n",
    "     vectors=embeddings\n",
    "     )\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Querying the database\n",
    "Ultimately we want to obtain a ranking of the documents most relevant to the user.\n",
    "<p style=\"text-align:center;\">\n",
    "<img src=\"https://docs.aleph-alpha.com/assets/images/ranking-0dfb759919d10d3f122869fc8eb09b2a.png\" class=\"center\" width=500>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Most relevant text: The Makeathon seeks to achieve this by providing talks of leading industry speakers, hands-on \n",
       "challenges, networking opportunities with like-minded people, industry leaders, start-ups and research \n",
       "organizations. This year's Makeathon is in-person on TUM's campus in Garching during the last week of April. We \n",
       "invite you to explore the possibilities of AI and create innovative solutions that can positively impact everyone.\n",
       "\n",
       "Highest Score: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.7864007526946204</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Most relevant text: The Makeathon seeks to achieve this by providing talks of leading industry speakers, hands-on \n",
       "challenges, networking opportunities with like-minded people, industry leaders, start-ups and research \n",
       "organizations. This year's Makeathon is in-person on TUM's campus in Garching during the last week of April. We \n",
       "invite you to explore the possibilities of AI and create innovative solutions that can positively impact everyone.\n",
       "\n",
       "Highest Score: \u001b[1;36m0.7864007526946204\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "client = Client(token=os.getenv(\"AA_TOKEN\"))\n",
    "\n",
    "query_text = \"Where is the makeathon?\"\n",
    "\n",
    "embedding_params = {\n",
    "        \"prompt\": Prompt.from_text(query_text),\n",
    "        \"representation\": SemanticRepresentation.Query,\n",
    "        \"compress_to_size\": 128,\n",
    "    }\n",
    "req = SemanticEmbeddingRequest(**embedding_params)\n",
    "response = client.semantic_embed(req, model=\"luminous-base\")\n",
    "\n",
    "query_embedding = response.embedding\n",
    "\n",
    "# retrieve the three most relevant documents\n",
    "search_result = q_client.search(\n",
    "    collection_name=\"test_collection\",\n",
    "    query_vector=query_embedding, \n",
    "    limit=3\n",
    ")\n",
    "\n",
    "print(f\"\"\"Most relevant text: {search_result[0].payload[\"text\"]}\\n\\nHighest Score: {search_result[0].score}\"\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the most relevant document to generate an answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Completion: The makeathon is in-person on TUM's campus in Garching during the last week of April.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Completion: The makeathon is in-person on TUM's campus in Garching during the last week of April.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from aleph_alpha_client import CompletionRequest\n",
    "\n",
    "prompt_text = f\"\"\"Answer the question based on the context.\n",
    "\n",
    "Context: {search_result[0].payload[\"text\"]}\n",
    "\n",
    "Q: {query_text}\n",
    "A:\"\"\"\n",
    "\n",
    "params = {\n",
    "    \"prompt\": Prompt.from_text(prompt_text),\n",
    "    \"maximum_tokens\": 100,\n",
    "    \"stop_sequences\": [\"\\n\"],\n",
    "}\n",
    "request = CompletionRequest(**params)\n",
    "response = client.complete(request=request, model=\"luminous-supreme\")\n",
    "completion = response.completions[0].completion\n",
    "\n",
    "print(\n",
    "    f\"\"\"Completion: {completion.strip()}\"\"\"\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explaining the output to the user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">EXPLAINED TEXT: Answer the question based on the context.\n",
       "Score: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.235</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "EXPLAINED TEXT: Answer the question based on the context.\n",
       "Score: \u001b[1;36m-1.235\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">EXPLAINED TEXT: Context: The Makeathon seeks to achieve this by providing talks of leading industry speakers, \n",
       "hands-on challenges, networking opportunities with like-minded people, industry leaders, start-ups and research \n",
       "organizations.\n",
       "Score: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.001</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "EXPLAINED TEXT: Context: The Makeathon seeks to achieve this by providing talks of leading industry speakers, \n",
       "hands-on challenges, networking opportunities with like-minded people, industry leaders, start-ups and research \n",
       "organizations.\n",
       "Score: \u001b[1;36m-1.001\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">EXPLAINED TEXT: This year's Makeathon is in-person on TUM's campus in Garching during the last week of April.\n",
       "Score: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">18.78</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "EXPLAINED TEXT: This year's Makeathon is in-person on TUM's campus in Garching during the last week of April.\n",
       "Score: \u001b[1;36m18.78\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">EXPLAINED TEXT: We invite you to explore the possibilities of AI and create innovative solutions that can \n",
       "positively impact everyone.\n",
       "Score: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.558</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "EXPLAINED TEXT: We invite you to explore the possibilities of AI and create innovative solutions that can \n",
       "positively impact everyone.\n",
       "Score: \u001b[1;36m-0.558\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">EXPLAINED TEXT: Q: Where is the makeathon?\n",
       "Score: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.507</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "EXPLAINED TEXT: Q: Where is the makeathon?\n",
       "Score: \u001b[1;36m3.507\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">EXPLAINED TEXT: A:\n",
       "Score: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.545</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "EXPLAINED TEXT: A:\n",
       "Score: \u001b[1;36m0.545\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from aleph_alpha_client import ExplanationRequest\n",
    "import numpy as np\n",
    "\n",
    "exp_req = ExplanationRequest(Prompt.from_text(prompt_text), completion, prompt_granularity=\"sentence\")\n",
    "response_explain = client.explain(exp_req, model=\"luminous-supreme\")\n",
    "\n",
    "explanations = response_explain[1][0].items[0][0]\n",
    "\n",
    "for item in explanations:\n",
    "    start = item.start\n",
    "    end = item.start + item.length\n",
    "    print(f\"\"\"EXPLAINED TEXT: {prompt_text[start:end]}\n",
    "Score: {np.round(item.score, decimals=3)}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aa_client_v3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
